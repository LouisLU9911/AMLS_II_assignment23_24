{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 180\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "NUM_WORKERS = 15\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin loading /home/uceezl8/amls_ii/AMLS_II_assignment23_24/Datasets/imagefolder ...\n",
      "dataset setup successfully!\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "train_image_folder = os.path.join(cwd, \"Datasets\", \"imagefolder\")\n",
    "\n",
    "print(f\"begin loading {train_image_folder} ...\")\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=train_image_folder)\n",
    "print(\"dataset setup successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"louislu9911/convnextv2-base-1k-224-finetuned-cassava-leaf-disease\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_checkpoint = \"facebook/convnextv2-base-1k-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(pre_trained_checkpoint)\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(crop_size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.1, seed=RANDOM_SEED)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x600>,\n",
       " 'label': 1,\n",
       " 'pixel_values': tensor([[[-1.0390, -1.1932, -1.2959,  ..., -0.4054, -0.7137, -0.8678],\n",
       "          [-0.7479, -1.0048, -1.3473,  ...,  0.2453,  0.4679,  0.1597],\n",
       "          [-1.0219, -0.8335, -0.9363,  ...,  0.4166,  0.7591,  0.5364],\n",
       "          ...,\n",
       "          [-0.8849, -0.0629,  0.3994,  ...,  0.9817,  1.0331,  0.9646],\n",
       "          [-0.3712,  0.9132,  1.0159,  ...,  0.8618,  1.1700,  0.8789],\n",
       "          [ 0.0741,  1.5639,  1.6495,  ...,  1.1015,  1.2214,  0.8618]],\n",
       " \n",
       "         [[-1.1954, -1.0728, -1.1604,  ..., -0.5476, -0.8452, -0.9678],\n",
       "          [-1.2829, -1.2479, -1.4055,  ...,  0.1001,  0.3452,  0.0476],\n",
       "          [-1.6856, -1.4405, -1.3704,  ...,  0.2752,  0.6604,  0.4328],\n",
       "          ...,\n",
       "          [-0.8452, -0.1099,  0.3102,  ...,  0.8004,  0.8704,  0.8179],\n",
       "          [-0.3375,  0.8354,  0.9055,  ...,  0.6954,  1.0105,  0.7304],\n",
       "          [ 0.1001,  1.5007,  1.5357,  ...,  0.9755,  1.0805,  0.6954]],\n",
       " \n",
       "         [[-0.9156, -0.8981, -0.9853,  ..., -0.3927, -0.6541, -0.7413],\n",
       "          [-0.9156, -0.9504, -1.1421,  ...,  0.1999,  0.4788,  0.1999],\n",
       "          [-1.3339, -1.0550, -0.9853,  ...,  0.3045,  0.6879,  0.4788],\n",
       "          ...,\n",
       "          [-1.5779, -0.7413, -0.1835,  ...,  0.5136,  0.5485,  0.5485],\n",
       "          [-1.0376,  0.2696,  0.4788,  ...,  0.4265,  0.7228,  0.4962],\n",
       "          [-0.4973,  1.0888,  1.2282,  ...,  0.7402,  0.8274,  0.4962]]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConvNextV2ForImageClassification(\n  (convnextv2): ConvNextV2Model(\n    (embeddings): ConvNextV2Embeddings(\n      (patch_embeddings): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n      (layernorm): ConvNextV2LayerNorm()\n    )\n    (encoder): ConvNextV2Encoder(\n      (stages): ModuleList(\n        (0): ConvNextV2Stage(\n          (downsampling_layer): Identity()\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n        (1): ConvNextV2Stage(\n          (downsampling_layer): Sequential(\n            (0): ConvNextV2LayerNorm()\n            (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n        (2): ConvNextV2Stage(\n          (downsampling_layer): Sequential(\n            (0): ConvNextV2LayerNorm()\n            (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (3): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (4): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (5): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (6): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (7): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (8): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (9): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (10): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (11): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (12): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (13): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (14): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (15): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (16): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (17): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (18): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (19): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (20): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (21): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (22): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (23): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (24): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (25): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (26): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n        (3): ConvNextV2Stage(\n          (downsampling_layer): Sequential(\n            (0): ConvNextV2LayerNorm()\n            (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n      )\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n) argument after ** must be a mapping, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mval_ds[:][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\u001b[0;31mTypeError\u001b[0m: ConvNextV2ForImageClassification(\n  (convnextv2): ConvNextV2Model(\n    (embeddings): ConvNextV2Embeddings(\n      (patch_embeddings): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n      (layernorm): ConvNextV2LayerNorm()\n    )\n    (encoder): ConvNextV2Encoder(\n      (stages): ModuleList(\n        (0): ConvNextV2Stage(\n          (downsampling_layer): Identity()\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n        (1): ConvNextV2Stage(\n          (downsampling_layer): Sequential(\n            (0): ConvNextV2LayerNorm()\n            (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n        (2): ConvNextV2Stage(\n          (downsampling_layer): Sequential(\n            (0): ConvNextV2LayerNorm()\n            (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (3): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (4): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (5): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (6): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (7): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (8): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (9): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (10): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (11): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (12): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (13): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (14): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (15): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (16): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (17): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (18): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (19): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (20): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (21): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (22): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (23): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (24): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (25): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n            (26): ConvNextV2Layer(\n              (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n        (3): ConvNextV2Stage(\n          (downsampling_layer): Sequential(\n            (0): ConvNextV2LayerNorm()\n            (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (layers): Sequential(\n            (0): ConvNextV2Layer(\n              (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n              (drop_path): Identity()\n            )\n            (1): ConvNextV2Layer(\n              (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n              (drop_path): Identity()\n            )\n            (2): ConvNextV2Layer(\n              (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n              (layernorm): ConvNextV2LayerNorm()\n              (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n              (act): GELUActivation()\n              (grn): ConvNextV2GRN()\n              (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n              (drop_path): Identity()\n            )\n          )\n        )\n      )\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n) argument after ** must be a mapping, not list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**val_ds[:]['pixel_values'])\n",
    "    logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-cassava-leaf-disease\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1969643/1184057695.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, test_y_pred, _ = trainer.predict(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = val_ds[:]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 116,    0,    0,    0,    0],\n",
       "       [   0,  231,    0,    0,    0],\n",
       "       [   0,    0,  244,    0,    0],\n",
       "       [   0,    0,    0, 1301,    0],\n",
       "       [   0,    0,    0,    0,  248]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, test_y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19257"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = []\n",
    "for i in train_ds:\n",
    "    train_y.append(i['label'])\n",
    "len(train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.convnextv2.modeling_convnextv2.ConvNextV2ForImageClassification"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, train_y_pred, _ = trainer.predict(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  971,     0,     0,     0,     0],\n",
       "       [    0,  1958,     0,     0,     0],\n",
       "       [    0,     0,  2142,     0,     0],\n",
       "       [    0,     0,     0, 11857,     0],\n",
       "       [    0,     0,     0,     0,  2329]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_y, train_y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='203' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [203/260 30:14 < 08:34, 0.11 it/s, Epoch 14.96/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.574900</td>\n",
       "      <td>2.582166</td>\n",
       "      <td>0.491121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.776300</td>\n",
       "      <td>0.606950</td>\n",
       "      <td>0.784112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.494900</td>\n",
       "      <td>0.431343</td>\n",
       "      <td>0.846262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.390270</td>\n",
       "      <td>0.862617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.385523</td>\n",
       "      <td>0.869626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.348935</td>\n",
       "      <td>0.878505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.345696</td>\n",
       "      <td>0.882710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.335924</td>\n",
       "      <td>0.882243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/uceezl8/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:595] . unexpected pos 503262976 vs 503262864",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/serialization.py:863\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    862\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 863\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/527: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBegin training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining ends\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# rest is optional but nice to have\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/transformers/trainer.py:1530\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1537\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/transformers/trainer.py:1944\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/transformers/trainer.py:2302\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2299\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/transformers/trainer.py:2382\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(staging_output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2381\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 2382\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[1;32m   2384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(staging_output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/transformers/trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   2494\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   2495\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   2496\u001b[0m     )\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2499\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   2503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   2504\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/serialization.py:630\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m--> 630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m~/.conda/envs/amls_ii-final-uceezl8/lib/python3.9/site-packages/torch/serialization.py:476\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:595] . unexpected pos 503262976 vs 503262864"
     ]
    }
   ],
   "source": [
    "print(\"Begin training...\")\n",
    "train_results = trainer.train()\n",
    "print(\"training ends\")\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amls_ii-final-uceezl8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
